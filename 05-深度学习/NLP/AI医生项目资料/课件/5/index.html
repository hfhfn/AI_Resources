<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>第五章:命名实体审核任务 - DOCTOR</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u7b2c\u4e94\u7ae0:\u547d\u540d\u5b9e\u4f53\u5ba1\u6838\u4efb\u52a1";
    var mkdocs_page_input_path = "5.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-36723568-3', 'mkdocs.org');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DOCTOR</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../1/">第一章:背景介绍与Unit的使用</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../2/">第二章:在线医生的总体架构与工具介绍</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../3/">第三章:neo4j图数据库</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../4/">第四章:离线部分</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">第五章:命名实体审核任务</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#51">5.1 任务介绍与模型选用</a></li>
    

    <li class="toctree-l2"><a href="#52">5.2 训练数据集</a></li>
    

    <li class="toctree-l2"><a href="#53-bert">5.3 BERT中文预训练模型</a></li>
    

    <li class="toctree-l2"><a href="#54-rnn">5.4 构建RNN模型</a></li>
    

    <li class="toctree-l2"><a href="#55">5.5 进行模型训练</a></li>
    

    <li class="toctree-l2"><a href="#56">5.6 模型使用</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../6/">第六章:命名实体识别任务</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../7/">第七章:在线部分</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../8/">第八章:句子主题相关任务</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../9/">第九章:系统联调与测试</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../10/">附录:环境安装部署手册</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DOCTOR</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>第五章:命名实体审核任务</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="51">5.1 任务介绍与模型选用</h2>
<ul>
<li>学习目标:<ul>
<li>了解命名实体审核任务的相关知识.</li>
<li>了解选用的模型及其原因.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>NE审核任务: <ul>
<li>一般在实体进入数据库存储前, 中间都会有一道必不可少的工序, 就是对识别出来的实体进行合法性的检验, 即命名实体(NE)审核任务. 它的检验过程不使用上下文信息, 更关注于字符本身的组合方式来进行判断, 本质上，它是一项短文本二分类问题.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>选用的模型及其原因:<ul>
<li>针对短文本任务, 无须捕捉长距离的关系, 因此我们使用了传统的RNN模型来解决, 性能和效果可以达到很好的均衡.</li>
<li>短文本任务往往适合使用字嵌入的方式, 但是如果你的训练集不是很大,涉及的字数有限, 那么可以直接使用预训练模型的字向量进行表示即可. 我们这里使用了bert-chinese预训练模型来获得中文汉字的向量表示.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="52">5.2 训练数据集</h2>
<ul>
<li>学习目标:<ul>
<li>了解训练数据集的样式及其相关解释.</li>
<li>掌握将数据集加载到内存中的过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>训练数据集的样式:</li>
</ul>
<pre><code>1   手内肌萎缩
0   缩萎肌内手
1   尿黑酸
0   酸黑尿
1   单眼眼前黑影
0   影黑前眼眼单
1   忧郁
0   郁忧
1   红细胞寿命缩短
0   短缩命寿胞细红
1   皮肤黏蛋白沉积
0   积沉白蛋黏肤皮
1   眼神异常
0   常异神眼
1   阴囊坠胀痛
0   痛胀坠囊阴
1   动脉血氧饱和度降低
0   低降度和饱氧血脉动
</code></pre>

<hr />
<ul>
<li>数据集的相关解释:<ul>
<li>这些训练集中的正样本往往是基于人工审核的标准命名实体.</li>
<li>数据集中的第一列代表标签, 1为正标签, 代表后面的文字是命名实体. 0为负标签, 代表后面的文字不是命名实体.  </li>
<li>数据集中的第二列中的命名实体来源于数据库中的症状实体名字, 它是结构化爬虫抓取的数据. 而非命名实体则是它的字符串反转.</li>
<li>正负样本的比例是1:1. </li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>将数据集加载到内存:</li>
</ul>
<pre><code>import pandas as pd 
from collections import Counter

# 读取数据
train_data_path = &quot;./train_data.csv&quot;
train_data= pd.read_csv(train_data_path, header=None, sep=&quot;\t&quot;)

# 打印正负标签比例
print(dict(Counter(train_data[0].values)))

# 转换数据到列表形式
train_data = train_data.values.tolist()
print(train_data[:10])
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/train.py</li>
</ul>
<hr />
<ul>
<li>输出效果:</li>
</ul>
<pre><code># 正负标签比例
{1: 5740, 0: 5740}

# 取出10条训练数据查看
[[1, '枕部疼痛'], [0, '痛疼部枕'], [1, '陶瑟征阳性'], [0, '性阳征瑟陶'], [1, '恋兽型性变态'], [0, '态变性型兽恋'], [1, '进食困难'], [0, '难困食进'], [1, '会阴瘘管或窦道形成'], [0, '成形道窦或管瘘阴会']]

</code></pre>

<hr />
<ul>
<li>小节总结:<ul>
<li>学习了训练数据集的样式及其相关解释.</li>
<li>学习了将数据集加载到内存中的过程.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="53-bert">5.3 BERT中文预训练模型</h2>
<ul>
<li>学习目标:<ul>
<li>了解BERT中文预训练模型的有关知识和作用.</li>
<li>掌握使用BERT中文预训练模型对句子编码的过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>BERT中文预训练模型:<ul>
<li>BERT模型整体架构基于Transformer模型架构, BERT中文预训练模型的解码器和编码器具有12层, 输出层中的线性层具有768个节点, 即输出张量最后一维的维度是768. 它使用的多头注意力机制结构中, 头的数量为12, 模型总参数量为110M. 同时, 它在中文简体和繁体上进行训练, 因此适合中文简体和繁体任务.  </li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>BERT中文预训练模型作用:<ul>
<li>在实际的文本任务处理中, 有些训练语料很难获得, 他们的总体数量和包含的词汇总数都非常少, 不适合用于训练带有Embedding层的模型, 但这些数据中却又蕴含这一些有价值的规律可以被模型挖掘, 在这种情况下,使用预训练模型对原始文本进行编码是非常不错的选择, 因为预训练模型来自大型语料, 能够使得当前文本具有意义, 虽然这些意义可能并不针对某个特定领域, 但是这种缺陷可以使用微调模型来进行弥补.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>使用BERT中文预训练模型对句子编码:</li>
</ul>
<pre><code>import torch
import torch.nn as nn


# 通过torch.hub(pytorch中专注于迁移学的工具)获得已经训练好的bert-base-chinese模型
model =  torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-chinese')


# 获得对应的字符映射器, 它将把中文的每个字映射成一个数字
tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-chinese')


def get_bert_encode_for_single(text):
    &quot;&quot;&quot;
    description: 使用bert-chinese编码中文文本
    :param text: 要进行编码的文本
    :return: 使用bert编码后的文本张量表示
    &quot;&quot;&quot;
    # 首先使用字符映射器对每个汉字进行映射
    # 这里需要注意, bert的tokenizer映射后会为结果前后添加开始和结束标记即101和102 
    # 这对于多段文本的编码是有意义的, 但在我们这里没有意义, 因此使用[1:-1]对头和尾进行切片
    indexed_tokens = tokenizer.encode(text)[1:-1]
    # 之后将列表结构转化为tensor
    tokens_tensor = torch.tensor([indexed_tokens])
    print(tokens_tensor)
    # 使模型不自动计算梯度
    with torch.no_grad():
        # 调用模型获得隐层输出
        encoded_layers, _ = model(tokens_tensor)
    # 输出的隐层是一个三维张量, 最外层一维是1, 我们使用[0]降去它.
    print(encoded_layers.shape)
    encoded_layers = encoded_layers[0]
    return encoded_layers
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/bert_chinese_encode.py</li>
</ul>
<hr />
<ul>
<li>输入参数:</li>
</ul>
<pre><code>text = &quot;你好, 周杰伦&quot;
</code></pre>

<hr />
<ul>
<li>调用:</li>
</ul>
<pre><code>outputs = get_bert_encode_for_single(text)
print(outputs)
print(outputs.shape)
</code></pre>

<hr />
<ul>
<li>输出效果:</li>
</ul>
<pre><code>tensor([[ 3.2731e-01, -1.4832e-01, -9.1618e-01,  ..., -4.4088e-01,
         -4.1074e-01, -7.5570e-01],
        [-1.1287e-01, -7.6269e-01, -6.4861e-01,  ..., -8.0478e-01,
         -5.3600e-01, -3.1953e-01],
        [-9.3012e-02, -4.4381e-01, -1.1985e+00,  ..., -3.6624e-01,
         -4.7467e-01, -2.6408e-01],
        [-1.6896e-02, -4.3753e-01, -3.6060e-01,  ..., -3.2451e-01,
         -3.4204e-02, -1.7930e-01],
        [-1.3159e-01, -3.0048e-01, -2.4193e-01,  ..., -4.5756e-02,
         -2.0958e-01, -1.0649e-01],
        [-4.0006e-01, -3.4410e-01, -3.8532e-05,  ...,  1.9081e-01,
          1.7006e-01, -3.6221e-01]])

torch.Size([6, 768])
</code></pre>

<hr />
<ul>
<li>
<p>小节总结:</p>
<ul>
<li>学习了BERT中文预训练模型的有关知识:<ul>
<li>BERT模型整体架构基于Transformer模型架构, BERT中文预训练模型的解码器和编码器具有12层, 输出层中的线性层具有768个节点, 即输出张量最后一维的维度是768. 它使用的多头注意力机制结构中, 头的数量为12, 模型总参数量为110M. 同时, 它在中文简体和繁体上进行训练, 因此适合中文简体和繁体任务.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>学习了BERT中文预训练模型的作用:<ul>
<li>在实际的文本任务处理中, 有些训练语料很难获得, 他们的总体数量和包含的词汇总数都非常少, 不适合用于训练带有Embedding层的模型, 但这些数据中却又蕴含这一些有价值的规律可以被模型挖掘, 在这种情况下, 使用预训练模型对原始文本进行编码是非常不错的选择, 因为预训练模型来自大型语料, 能够使得当前文本具有意义, 虽然这些意义可能并不针对某个特定领域, 但是这种缺陷可以使用微调模型来进行弥补.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>学习了使用BERT中文预训练模型对句子编码的函数: get_bert_encode_for_single(text)</li>
</ul>
</li>
</ul>
<hr />
<h2 id="54-rnn">5.4 构建RNN模型</h2>
<ul>
<li>学习目标:<ul>
<li>学习RNN模型的内部结构及计算公式.</li>
<li>掌握RNN模型的实现过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>传统RNN的内部结构图:</li>
</ul>
<p><center><img alt="avatar" src="../img/RNN内部结构图.png" /></center></p>
<hr />
<ul>
<li>结构解释图:</li>
</ul>
<p><center><img alt="avatar" src="../img/结构解释图.png" /></center></p>
<hr />
<ul>
<li>内部结构分析:
        * 我们把目光集中在中间的方块部分, 它的输入有两部分, 分别是h(t-1)以及x(t), 代表上一时间步的隐层输出, 以及此时间步的输入, 它们进入RNN结构体后, 会"融合"到一起, 这种融合我们根据结构解释可知, 是将二者进行拼接, 形成新的张量[x(t), h(t-1)], 之后这个新的张量将通过一个全连接层(线性层), 该层&gt;使用tanh作为激活函数, 最终得到该时间步的输出h(t), 它将作为下一个时间步的&gt;输入和x(t+1)一起进入结构体. 以此类推.</li>
</ul>
<hr />
<ul>
<li>内部结构过程演示:</li>
</ul>
<p><center><img alt="avatar" src="../img/RNN结构过程图.gif" /></center></p>
<hr />
<ul>
<li>根据结构分析得出内部计算公式:</li>
</ul>
<p><center><img alt="avatar" src="../img/RNN公式图.png" /></center></p>
<hr />
<ul>
<li>激活函数tanh的作用:
        * 用于帮助调节流经网络的值, tanh函数将值压缩在-1和1之间.</li>
</ul>
<hr />
<p><center><img alt="avatar" src="../img/tanh激活函数.gif" /></center></p>
<hr />
<ul>
<li>构建RNN模型的代码分析:</li>
</ul>
<pre><code>class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        &quot;&quot;&quot;初始化函数中有三个参数,分别是输入张量最后一维的尺寸大小,
 隐层张量最后一维的尺寸大小, 输出张量最后一维的尺寸大小&quot;&quot;&quot;
        super(RNN, self).__init__()
        # 传入隐含层尺寸大小
        self.hidden_size = hidden_size
        # 构建从输入到隐含层的线性变化, 这个线性层的输入尺寸是input_size + hidden_size
        # 这是因为在循环网络中, 每次输入都有两部分组成，分别是此时刻的输入和上一时刻产生的输出.
        # 这个线性层的输出尺寸是hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        # 构建从输入到输出层的线性变化, 这个线性层的输入尺寸还是input_size + hidden_size
        # 这个线性层的输出尺寸是output_size.
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        # 最后需要对输出做softmax处理, 获得结果.
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, input, hidden):
        &quot;&quot;&quot;在forward函数中, 参数分别是规定尺寸的输入张量, 以及规定尺寸的初始化隐层张量&quot;&quot;&quot;
        # 首先使用torch.cat将input与hidden进行张量拼接
        combined = torch.cat((input, hidden), 1)
        # 通过输入层到隐层变换获得hidden张量
        hidden = self.i2h(combined)
        # 通过输入到输出层变换获得output张量
        output = self.i2o(combined)
        # 对输出进行softmax处理
        output = self.softmax(output)
        # 返回输出张量和最后的隐层结果
        return output, hidden

    def initHidden(self):
        &quot;&quot;&quot;隐层初始化函数&quot;&quot;&quot;
        # 将隐层初始化成为一个1xhidden_size的全0张量
        return torch.zeros(1, self.hidden_size)
</code></pre>

<hr />
<blockquote>
<ul>
<li>torch.cat演示:</li>
</ul>
</blockquote>
<pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
ensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,-1.0969, -0.4614],
       [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,-0.5790,  0.1497]])

</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/RNN_MODEL.py</li>
</ul>
<hr />
<ul>
<li>实例化参数:</li>
</ul>
<pre><code>input_size = 768
hidden_size = 128
n_categories = 2
</code></pre>

<hr />
<ul>
<li>输入参数:</li>
</ul>
<pre><code>input = torch.rand(1, input_size)
hidden = torch.rand(1, hidden_size)
</code></pre>

<hr />
<ul>
<li>调用:</li>
</ul>
<pre><code>from RNN_MODEL import RNN
rnn = RNN(input_size, hidden_size, n_categories)
outputs, hidden = rnn(input, hidden)
print(&quot;outputs:&quot;, outputs)
print(&quot;hidden:&quot;, hidden)
</code></pre>

<ul>
<li>输出效果:</li>
</ul>
<pre><code>outputs: tensor([[-0.7858, -0.6084]], grad_fn=&lt;LogSoftmaxBackward&gt;)

hidden: tensor([[-4.8444e-01, -5.9609e-02,  1.7870e-01, 
                 -1.6553e-01,  ... , 5.6711e-01]], grad_fn=&lt;AddmmBackward&gt;))
</code></pre>

<hr />
<ul>
<li>小节总结:<ul>
<li>学习了RNN模型的内部结构及计算公式.</li>
<li>学习并实现了RNN模型的类: class RNN(nn.Module).</li>
</ul>
</li>
</ul>
<hr />
<h2 id="55">5.5 进行模型训练</h2>
<ul>
<li>学习目标:<ul>
<li>了解进行模型训练的步骤.</li>
<li>掌握模型训练中每个步骤的实现过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>进行模型训练的步骤:<ul>
<li>第一步: 构建随机选取数据函数.</li>
<li>第二步: 构建模型训练函数.</li>
<li>第三步: 构建模型验证函数.</li>
<li>第四步: 调用训练和验证函数.</li>
<li>第五步: 绘制训练和验证的损失和准确率对照曲线.</li>
<li>第六步: 模型保存. </li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第一步: 构建随机选取数据函数 </li>
</ul>
<pre><code># 导入bert中文编码的预训练模型
from bert_chinese_encode import get_bert_encode_for_single
def randomTrainingExample(train_data):
    &quot;&quot;&quot;随机选取数据函数, train_data是训练集的列表形式数据&quot;&quot;&quot;
    # 从train_data随机选择一条数据
    category, line = random.choice(train_data)
    # 将里面的文字使用bert进行编码, 获取编码后的tensor类型数据
    line_tensor = get_bert_encode_for_single(line)
    # 将分类标签封装成tensor
    category_tensor = torch.tensor([int(category)])
    # 返回四个结果
    return category, line, category_tensor, line_tensor

</code></pre>

<hr />
<blockquote>
<ul>
<li>代码位置:  /data/doctor_offline/review_model/train.py</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<pre><code># 将数据集加载到内存获得的train_data
</code></pre>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<pre><code># 选择10条数据进行查看
for i in range(10):
    category, line, category_tensor, line_tensor = randomTrainingExample(train_data)
    print('category =', category, '/ line =', line)

</code></pre>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<pre><code>category = 1 / line = 触觉失调
category = 0 / line = 颤震性理生
category = 0 / line = 征压血高娠妊
category = 1 / line = 食欲减退
category = 0 / line = 血淤道肠胃
category = 0 / line = 形畸节关
category = 0 / line = 咳呛水饮
category = 0 / line = 症痣巨
category = 1 / line = 昼盲
category = 1 / line = 眼神异常
</code></pre>

<hr />
<ul>
<li>第二步: 构建模型训练函数</li>
</ul>
<pre><code># 选取损失函数为NLLLoss()
criterion = nn.NLLLoss()
# 学习率为0.005
learning_rate = 0.005


def train(category_tensor, line_tensor):
    &quot;&quot;&quot;模型训练函数, category_tensor代表类别张量, line_tensor代表编码后的文本张量&quot;&quot;&quot;
    # 初始化隐层 
    hidden = rnn.initHidden()
    # 模型梯度归0
    rnn.zero_grad()
    # 遍历line_tensor中的每一个字的张量表示
    for i in range(line_tensor.size()[0]):
        # 然后将其输入到rnn模型中, 因为模型要求是输入必须是二维张量, 因此需要拓展一个维度, 循环调用rnn直到最后一个字
        output, hidden = rnn(line_tensor[i].unsqueeze(0), hidden)
    # 根据损失函数计算损失, 输入分别是rnn的输出结果和真正的类别标签
    loss = criterion(output, category_tensor)
    # 将误差进行反向传播
    loss.backward()

    # 更新模型中所有的参数
    for p in rnn.parameters():
        # 将参数的张量表示与参数的梯度乘以学习率的结果相加以此来更新参数
        p.data.add_(-learning_rate, p.grad.data)

    # 返回结果和损失的值
    return output, loss.item()
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/train.py</li>
</ul>
<hr />
<ul>
<li>第三步: 模型验证函数</li>
</ul>
<pre><code>def valid(category_tensor, line_tensor):
    &quot;&quot;&quot;模型验证函数, category_tensor代表类别张量, line_tensor代表编码后的文本张量&quot;&quot;&quot;
    # 初始化隐层
    hidden = rnn.initHidden()
    # 验证模型不自动求解梯度
    with torch.no_grad():
        # 遍历line_tensor中的每一个字的张量表示    
        for i in range(line_tensor.size()[0]):
            # 然后将其输入到rnn模型中, 因为模型要求是输入必须是二维张量, 因此需要拓展一个维度, 循环调用rnn直到最后一个字
            output, hidden = rnn(line_tensor[i].unsqueeze(0), hidden)      
        # 获得损失
        loss = criterion(output, category_tensor)
     # 返回结果和损失的值
    return output, loss.item()

</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/train.py</li>
</ul>
<hr />
<ul>
<li>第四步: 调用训练和验证函数</li>
</ul>
<blockquote>
<ul>
<li>构建时间计算函数: </li>
</ul>
</blockquote>
<pre><code>import time
import math

def timeSince(since):
    &quot;获得每次打印的训练耗时, since是训练开始时间&quot;
    # 获得当前时间
    now = time.time()
    # 获得时间差，就是训练耗时
    s = now - since
    # 将秒转化为分钟, 并取整
    m = math.floor(s / 60)
    # 计算剩下不够凑成1分钟的秒数
    s -= m * 60
    # 返回指定格式的耗时
    return '%dm %ds' % (m, s)
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/train.py</li>
</ul>
<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<pre><code># 假定模型训练开始时间是10min之前
since = time.time() - 10*60
</code></pre>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<pre><code>period = timeSince(since)
print(period)
</code></pre>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<pre><code>10m 0s
</code></pre>

<hr />
<blockquote>
<ul>
<li>调用训练和验证函数并打印日志</li>
</ul>
</blockquote>
<pre><code># 设置迭代次数为50000步
n_iters = 50000

# 打印间隔为1000步
plot_every = 1000


# 初始化打印间隔中训练和验证的损失和准确率
train_current_loss = 0
train_current_acc = 0
valid_current_loss = 0
valid_current_acc = 0


# 初始化盛装每次打印间隔的平均损失和准确率
all_train_losses = []
all_train_acc = []
all_valid_losses = []
all_valid_acc = []

# 获取开始时间戳
start = time.time()


# 循环遍历n_iters次 
for iter in range(1, n_iters + 1):
    # 调用两次随机函数分别生成一条训练和验证数据
    category, line, category_tensor, line_tensor = randomTrainingExample(train_data)
    category_, line_, category_tensor_, line_tensor_ = randomTrainingExample(train_data)
    # 分别调用训练和验证函数, 获得输出和损失
    train_output, train_loss = train(category_tensor, line_tensor)
    valid_output, valid_loss = valid(category_tensor_, line_tensor_)
    # 进行训练损失, 验证损失，训练准确率和验证准确率分别累加
    train_current_loss += train_loss
    train_current_acc += (train_output.argmax(1) == category_tensor).sum().item()
    valid_current_loss += valid_loss
    valid_current_acc += (valid_output.argmax(1) == category_tensor_).sum().item()
    # 当迭代次数是指定打印间隔的整数倍时
    if iter % plot_every == 0:
        # 用刚刚累加的损失和准确率除以间隔步数得到平均值
        train_average_loss = train_current_loss / plot_every
        train_average_acc = train_current_acc/ plot_every
        valid_average_loss = valid_current_loss / plot_every
        valid_average_acc = valid_current_acc/ plot_every
        # 打印迭代步, 耗时, 训练损失和准确率, 验证损失和准确率
        print(&quot;Iter:&quot;, iter, &quot;|&quot;, &quot;TimeSince:&quot;, timeSince(start))
        print(&quot;Train Loss:&quot;, train_average_loss, &quot;|&quot;, &quot;Train Acc:&quot;, train_average_acc)
        print(&quot;Valid Loss:&quot;, valid_average_loss, &quot;|&quot;, &quot;Valid Acc:&quot;, valid_average_acc)
        # 将结果存入对应的列表中，方便后续制图
        all_train_losses.append(train_average_loss)
        all_train_acc.append(train_average_acc)
        all_valid_losses.append(valid_average_loss)
        all_valid_acc.append(valid_average_acc)
        # 将该间隔的训练和验证损失及其准确率归0
        train_current_loss = 0
        train_current_acc = 0
        valid_current_loss = 0
        valid_current_acc = 0
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/train.py</li>
</ul>
<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<pre><code>Iter: 1000 | TimeSince: 0m 56s
Train Loss: 0.6127021567507527 | Train Acc: 0.747
Valid Loss: 0.6702297774022868 | Valid Acc: 0.7
Iter: 2000 | TimeSince: 1m 52s
Train Loss: 0.5190641692602076 | Train Acc: 0.789
Valid Loss: 0.5217500487511397 | Valid Acc: 0.784
Iter: 3000 | TimeSince: 2m 48s
Train Loss: 0.5398398997281778 | Train Acc: 0.8
Valid Loss: 0.5844468013737023 | Valid Acc: 0.777
Iter: 4000 | TimeSince: 3m 43s
Train Loss: 0.4700755337187358 | Train Acc: 0.822
Valid Loss: 0.5140456306522071 | Valid Acc: 0.802
Iter: 5000 | TimeSince: 4m 38s
Train Loss: 0.5260879981063878 | Train Acc: 0.804
Valid Loss: 0.5924804099237979 | Valid Acc: 0.796
Iter: 6000 | TimeSince: 5m 33s
Train Loss: 0.4702717279043861 | Train Acc: 0.825
Valid Loss: 0.6675750375208704 | Valid Acc: 0.78
Iter: 7000 | TimeSince: 6m 27s
Train Loss: 0.4734503294042624 | Train Acc: 0.833
Valid Loss: 0.6329268293256277 | Valid Acc: 0.784
Iter: 8000 | TimeSince: 7m 23s
Train Loss: 0.4258338176879665 | Train Acc: 0.847
Valid Loss: 0.5356959595441066 | Valid Acc: 0.82
Iter: 9000 | TimeSince: 8m 18s
Train Loss: 0.45773495503464817 | Train Acc: 0.843
Valid Loss: 0.5413714128659645 | Valid Acc: 0.798
Iter: 10000 | TimeSince: 9m 14s
Train Loss: 0.4856756244019302 | Train Acc: 0.835
Valid Loss: 0.5450502399195044 | Valid Acc: 0.813
</code></pre>

<hr />
<ul>
<li>第五步: 绘制训练和验证的损失和准确率对照曲线</li>
</ul>
<pre><code>import matplotlib.pyplot as plt

plt.figure(0)
plt.plot(all_train_losses, label=&quot;Train Loss&quot;)
plt.plot(all_valid_losses, color=&quot;red&quot;, label=&quot;Valid Loss&quot;)
plt.legend(loc='upper left')
plt.savefig(&quot;./loss.png&quot;)


plt.figure(1)
plt.plot(all_train_acc, label=&quot;Train Acc&quot;)
plt.plot(all_valid_acc, color=&quot;red&quot;, label=&quot;Valid Acc&quot;)
plt.legend(loc='upper left')
plt.savefig(&quot;./acc.png&quot;)
</code></pre>

<hr />
<blockquote>
<ul>
<li>代码位置:  /data/doctor_offline/review_model/train.py</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>训练和验证损失对照曲线:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="../img/rnn_loss.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>训练和验证准确率对照曲线:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="../img/rnn_acc.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>分析:<ul>
<li>损失对照曲线一直下降, 说明模型能够从数据中获取规律，正在收敛, 准确率对照曲线中验证准确率一直上升，最终维持在0.98左右.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>第六步: 模型保存</li>
</ul>
<pre><code># 保存路径
MODEL_PATH = './BERT_RNN.pth'
# 保存模型参数
torch.save(rnn.state_dict(), MODEL_PATH)
</code></pre>

<hr />
<blockquote>
<ul>
<li>代码位置:  /data/doctor_offline/review_model/train.py</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>输出效果: <ul>
<li>在/data/doctor_offline/review_model/路径下生成BERT_RNN.pth文件.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>小节总结:<ul>
<li>学习了进行模型训练的步骤:<ul>
<li>第一步: 构建随机选取数据函数.</li>
<li>第二步: 构建模型训练函数.</li>
<li>第三步: 构建模型验证函数.</li>
<li>第四步: 调用训练和验证函数.</li>
<li>第五步: 绘制训练和验证的损失和准确率对照曲线.</li>
<li>第六步: 模型保存.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h2 id="56">5.6 模型使用</h2>
<ul>
<li>学习目标:<ul>
<li>掌握模型预测的实现过程.</li>
<li>掌握模型批量预测的实现过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>模型预测的实现过程:</li>
</ul>
<pre><code>import os
import torch
import torch.nn as nn

# 导入RNN模型结构
from RNN_MODEL import RNN
# 导入bert预训练模型编码函数
from bert_chinese_encode import get_bert_encode_for_single


# 预加载的模型参数路径
MODEL_PATH = './BERT_RNN.pth'

# 隐层节点数, 输入层尺寸, 类别数都和训练时相同即可
n_hidden = 128
input_size = 768
n_categories = 2

# 实例化RNN模型, 并加载保存模型参数
rnn = RNN(input_size, n_hidden, n_categories)
rnn.load_state_dict(torch.load(MODEL_PATH))




def _test(line_tensor):
    &quot;&quot;&quot;模型测试函数, 它将用在模型预测函数中, 用于调用RNN模型并返回结果.它的参数line_tensor代表输入文本的张量表示&quot;&quot;&quot;
    # 初始化隐层张量
    hidden = rnn.initHidden()
    # 与训练时相同, 遍历输入文本的每一个字符
    for i in range(line_tensor.size()[0]):
        # 将其逐次输送给rnn模型
        output, hidden = rnn(line_tensor[i].unsqueeze(0), hidden)
    # 获得rnn模型最终的输出
    return output


def predict(input_line):
    &quot;&quot;&quot;模型预测函数, 输入参数input_line代表需要预测的文本&quot;&quot;&quot;
    # 不自动求解梯度
    with torch.no_grad():
        # 将input_line使用bert模型进行编码
        output = _test(get_bert_encode_for_single(input_line))
        # 从output中取出最大值对应的索引, 比较的维度是1
        _, topi = output.topk(1, 1)
        # 返回结果数值
        return topi.item()

</code></pre>

<hr />
<p>tensor.topk演示:</p>
<pre><code>&gt;&gt;&gt; tr = torch.randn(1, 2)
&gt;&gt;&gt; tr
tensor([[-0.1808, -1.4170]])
&gt;&gt;&gt; tr.topk(1, 1)
torch.return_types.topk(values=tensor([[-0.1808]]), indices=tensor([[0]]))
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/predict.py</li>
</ul>
<hr />
<ul>
<li>输入参数:</li>
</ul>
<pre><code>input_line = &quot;点瘀样尖针性发多&quot;
</code></pre>

<hr />
<ul>
<li>调用:</li>
</ul>
<pre><code>result = predict(input_line)
print(&quot;result:&quot;, result)
</code></pre>

<hr />
<ul>
<li>输出效果:</li>
</ul>
<pre><code>result: 0
</code></pre>

<hr />
<ul>
<li>模型批量预测的实现过程:</li>
</ul>
<pre><code>def batch_predict(input_path, output_path):
    &quot;&quot;&quot;批量预测函数, 以原始文本(待识别的命名实体组成的文件)输入路径
       和预测过滤后(去除掉非命名实体的文件)的输出路径为参数&quot;&quot;&quot;
    # 待识别的命名实体组成的文件是以疾病名称为csv文件名, 
    # 文件中的每一行是该疾病对应的症状命名实体
    # 读取路径下的每一个csv文件名, 装入csv列表之中
    csv_list = os.listdir(input_path)
    # 遍历每一个csv文件
    for csv in csv_list:
        # 以读的方式打开每一个csv文件
        with open(os.path.join(input_path, csv), &quot;r&quot;) as fr:
            # 再以写的方式打开输出路径的同名csv文件
            with open(os.path.join(output_path, csv), &quot;w&quot;) as fw:
                # 读取csv文件的每一行
                input_line = fr.readline()
                # 使用模型进行预测
                res = predict(input_line)
                # 如果结果为1
                if res:
                    # 说明审核成功, 写入到输出csv中
                    fw.write(input_line + &quot;\n&quot;)
                else:
                    pass
</code></pre>

<hr />
<ul>
<li>代码位置: /data/doctor_offline/review_model/predict.py</li>
</ul>
<hr />
<ul>
<li>输入参数:</li>
</ul>
<pre><code>input_path = &quot;/data/doctor_offline/structured/noreview/&quot;
output_path = &quot;/data/doctor_offline/structured/reviewed/&quot;
</code></pre>

<hr />
<ul>
<li>调用:</li>
</ul>
<pre><code>batch_predict(input_path, output_path)
</code></pre>

<hr />
<ul>
<li>输出效果:<ul>
<li>在输出路径下生成与输入路径等数量的同名csv文件, 内部的症状实体是被审核的可用实体.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>小节总结:<ul>
<li>学习并实现了模型预测的函数: predict(input_line).</li>
<li>学习并实现了模型批量预测的函数: batch_predict(input_path, output_path)</li>
</ul>
</li>
</ul>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../6/" class="btn btn-neutral float-right" title="第六章:命名实体识别任务">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../4/" class="btn btn-neutral" title="第四章:离线部分"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>©Copyright 2019, itcast.cn.</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../4/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../6/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
